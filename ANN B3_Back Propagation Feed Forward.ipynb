{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOfMeU/HriAsw2PX+UkaEaq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["B3. Write a python program for creating a Back Propagation Feed-forward neural network."],"metadata":{"id":"e32P5pFUaDLM"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zCUK-ydDaCMq","executionInfo":{"status":"ok","timestamp":1746548818775,"user_tz":-330,"elapsed":2272,"user":{"displayName":"SRUSHTI DHOTRE","userId":"06050880057213308556"}},"outputId":"3338bcc0-64a8-4f4a-fd31-bff96484eaed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 0.5012\n","Epoch 1000, Loss: 0.0789\n","Epoch 2000, Loss: 0.0420\n","Epoch 3000, Loss: 0.0314\n","Epoch 4000, Loss: 0.0259\n","Epoch 5000, Loss: 0.0225\n","Epoch 6000, Loss: 0.0202\n","Epoch 7000, Loss: 0.0184\n","Epoch 8000, Loss: 0.0170\n","Epoch 9000, Loss: 0.0159\n","\n","Final Output after Training:\n","[[0.01572886]\n"," [0.98627449]\n"," [0.98425486]\n"," [0.01450469]]\n"]}],"source":["import numpy as np\n","\n","def sigmoid(x):\t\t\t\t# Sigmoid activation function and its derivative\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    return x * (1 - x)\n","\n","# Initialize dataset (X: inputs, y: expected outputs)\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  \t\t\t\t\t# XOR input\n","y = np.array([[0], [1], [1], [0]])              \t\t\t\t\t# XOR output\n","\n","input_neurons = 2\t\t\t\t\t# Initialize neural network parameters\n","hidden_neurons = 4\n","output_neurons = 1\n","learning_rate = 0.5\n","epochs = 10000\n","\n","np.random.seed(42)\t\t\t\t        # Randomly initialize weights and biases\n","weights_input_hidden = np.random.uniform(-1, 1, (input_neurons, hidden_neurons))\n","weights_hidden_output = np.random.uniform(-1, 1, (hidden_neurons, output_neurons))\n","bias_hidden = np.random.uniform(-1, 1, (1, hidden_neurons))\n","bias_output = np.random.uniform(-1, 1, (1, output_neurons))\n","\n","for epoch in range(epochs):\t\t\t\t\t\t\t# Training process\n","\n","   \t\t\t\t\t\t\t\t                 # Forward propagation\n","    hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden\n","    hidden_layer_output = sigmoid(hidden_layer_input)\n","\n","    final_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n","    final_output = sigmoid(final_input)\n","\n","    error = y - final_output\t\t\t\t\t\t\t   # Compute error\n","\n","    d_output = error * sigmoid_derivative(final_output)\t\t            # Backpropagation\n","    error_hidden = d_output.dot(weights_hidden_output.T)\n","    d_hidden = error_hidden * sigmoid_derivative(hidden_layer_output)\n","\n","    \t\t\t\t\t\t\t                   # Update weights and biases\n","    weights_hidden_output += hidden_layer_output.T.dot(d_output) * learning_rate\n","    weights_input_hidden += X.T.dot(d_hidden) * learning_rate\n","    bias_output += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n","    bias_hidden += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n","\n","    if epoch % 1000 == 0:\t\t\t\t\t    # Print loss every 1000 epochs\n","        loss = np.mean(np.abs(error))\n","        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n","\n","print(\"\\nFinal Output after Training:\")\t\t\t\t # Testing trained model\n","print(final_output)"]},{"cell_type":"code","source":[],"metadata":{"id":"6zYf_BJfa88d"},"execution_count":null,"outputs":[]}]}