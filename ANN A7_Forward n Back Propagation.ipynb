{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO0EPXPIkxz8umVQLmFrHuG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["A7. Implement Artificial Neural Network training process in Python by using Forward Propagation, Back Propagation.\n"],"metadata":{"id":"CvtaUjbyVzpn"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ca6WJBLaVpsQ","executionInfo":{"status":"ok","timestamp":1746532363108,"user_tz":-330,"elapsed":554,"user":{"displayName":"SRUSHTI DHOTRE","userId":"06050880057213308556"}},"outputId":"bfb9b7c5-e965-4c46-926a-3b0616a5415d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Error: 0.4977550305860017\n","Epoch 1000, Error: 0.48962844155619734\n","Epoch 2000, Error: 0.430505591830237\n","Epoch 3000, Error: 0.335726373976126\n","Epoch 4000, Error: 0.17357496319517723\n","Epoch 5000, Error: 0.11181272498560173\n","Epoch 6000, Error: 0.08576413241547484\n","Epoch 7000, Error: 0.07130866479694536\n","Epoch 8000, Error: 0.061975191385776986\n","Epoch 9000, Error: 0.05537218409879135\n","\n","Trained output:\n","[[0.05322146]\n"," [0.95171535]\n"," [0.95160449]\n"," [0.05175396]]\n"]}],"source":["import numpy as np\n","\n","def sigmoid(x):\t\t\t\t\t                # Sigmoid activation function and its derivative\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    return x * (1 - x)\n","\n","X = np.array([[0, 0],\n","              [0, 1],\n","              [1, 0],\n","              [1, 1]])\t\t\t\t\t   # XOR input and output\n","\n","y = np.array([[0],\n","              [1],\n","              [1],\n","              [0]])\n","\n","np.random.seed(42)\t\t\t\t\t\t\t      # Initialize weights and biases\n","W1 = np.random.rand(2, 2)             # Input to hidden weights\n","b1 = np.random.rand(1, 2)             # Hidden layer bias\n","W2 = np.random.rand(2, 1)             # Hidden to output weights\n","b2 = np.random.rand(1, 1)             # Output layer bias\n","\n","lr = 0.1 \t\t\t\t\t\t\t\t\t\t            # Learning rate\n","\n","for epoch in range(10000):\t\t\t\t\t\t  # Training the neural network\n","\n","    hidden_input = np.dot(X, W1) + b1\t\t\t\t    \t\t# Forward pass\n","    hidden_output = sigmoid(hidden_input)\n","\n","    output_input = np.dot(hidden_output, W2) + b2\n","    output = sigmoid(output_input)\n","\n","    error = y - output\t\t\t\t\t\t\t\t    # Compute the error\n","\n","    d_output = error * sigmoid_derivative(output) \t\t\t\t       # Backpropagation\n","    d_hidden = d_output.dot(W2.T) * sigmoid_derivative(hidden_output)\n","\n","    W2 += hidden_output.T.dot(d_output) * lr \t\t                # Update weights and biases\n","    b2 += np.sum(d_output, axis=0, keepdims=True) * lr\n","    W1 += X.T.dot(d_hidden) * lr\n","    b1 += np.sum(d_hidden, axis=0, keepdims=True) * lr\n","\n","    if epoch % 1000 == 0:\t\t\t\t   \t        # Print error every 1000 iterations\n","        print(f\"Epoch {epoch}, Error: {np.mean(np.abs(error))}\")\n","\n","print(\"\\nTrained output:\")\t\t\t\t\t\t# Output the result after training\n","print(output)"]},{"cell_type":"code","source":[],"metadata":{"id":"SpD4-stOcNSp"},"execution_count":null,"outputs":[]}]}